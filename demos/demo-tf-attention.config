#!crnn/rnn.py
# kate: syntax python;

# task
use_tensorflow = True
task = "train"
device = "gpu"
multiprocessing = True
update_on_device = True

# data
num_inputs = 40  # Gammatone 40-dim
num_outputs = 9001 # 4501
EpochSplit = 6

# TODO...
train = get_sprint_dataset("train")
dev = get_sprint_dataset("cv")
cache_size = "0"
window = 1

# network
# (also defined by num_inputs & num_outputs)
num_outputs = {"data": [num_inputs,2], "classes": [num_outputs,1], "orth": {"dim": 256, "sparse": True, "dtype": "uint8"}}
network = {
"lstm0_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.0, "L2": 0.01, "direction": 1 },
"lstm0_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.0, "L2": 0.01, "direction": -1 },

"lstm1_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.0, "L2": 0.01, "direction": 1, "from" : ["lstm0_fw", "lstm0_bw"] },
"lstm1_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.0, "L2": 0.01, "direction": -1, "from" : ["lstm0_fw", "lstm0_bw"] },

"lstm2_fw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.0, "L2": 0.01, "direction": 1, "from" : ["lstm1_fw", "lstm1_bw"] },
"lstm2_bw" : { "class": "rec", "unit": "lstmp", "n_out" : 500, "dropout": 0.0, "L2": 0.01, "direction": -1, "from" : ["lstm1_fw", "lstm1_bw"] },

"encoder": {"class": "copy", "from": ["lstm2_fw", "lstm2_bw"]},
"enc_ctx": {"class": "linear", "activation": "tanh", "from": ["encoder"], "n_out": 100},

"output": {"class": "rec", "unit": {
    'orth_embed': {'class': 'linear', 'activation': None, 'from': ['data:orth'], "n_out": 50},
    "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["prev:c", "prev:orth_embed"], "n_out": 500},
    "c_in": {"class": "linear", "activation": "tanh", "from": ["s", "prev:orth_embed"], "n_out": 100},
    "c": {"class": "global_attention_context", "from": ["c_in"], "base": "base:encoder", "base_ctx": "base:enc_ctx", "n_out": 1000},
    "output": {"class": "softmax", "from": ["prev:s", "c"], "target": "orth"}
}, "target": "orth", "loss": "ce"}

}

# trainer
batching = "random"
batch_size = 5000
max_seqs = 40
chunking = "0"
truncation = -1
num_epochs = 100
pretrain = "default"
pretrain_construction_algo = "from_input"
gradient_clip = 0
adam = True
#gradient_noise = 0.3
learning_rate = 0.0005
learning_rate_file = "newbob.data"
learning_rate_control = "newbob_multi_epoch"
learning_rate_control_relative_error_relative_lr = True
newbob_multi_num_epochs = 6
newbob_multi_update_interval = 1
model = "net-model/network"

# log
log = "log/crnn.train.log"
log_verbosity = 5

